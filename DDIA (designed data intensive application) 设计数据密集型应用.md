2020 5-14
[toc]
#### 第一章：可靠性，可扩展性，可维护性
- 现代越来越多应用是数据密集型(**data intensive**),而不是计算密集型(**computing intensive**),相比于CPU，数据的传输，更新的操作越来越成为瓶颈。  
这里用**数据系统**这个抽象词，统称数据库，缓存，索引等一系列相关但又迥异的工具。因为他们的界限正在变得越来越模糊，例如redis用数据存储作为消息队列。尤其是，这些功能在设计时就需要组合在一起考虑，作为应用程序开发者，常常也要充当数据系统设计者。
- 一个应用有**功能需求(functional requirements)**（以什么方式完成搜索，存储，处理数据等任务）和**非功能需求(nonfunctional)**（如安全性，可靠性，可扩展性）
- 本书着重考虑三个方面：
  - **可靠性**(Reliability)：在困境中仍可正常工作
  - **可扩展性**(Scalability)：有合理的办法应对系统（数据量，流量，复杂性）的增长
  - **可维护性**(Maintainability)：许多不同的人（不同角色）在不同生命周期，都能高效地工作（维护系统，使其适应新的场景）  

##### 可靠性
- 可靠粗略的理解是“即使出现问题，也能正确工作”，这个问题，或者错误，即**故障(fault)**，对应的是容错。==容错只有针对特定类型的错误才有意义==，不可能可以容忍任何错误。故障不同于**失效(failure)**，故障是一部分状态偏离标准，失效是系统整体停止服务。故障的概率不可能为零，容错机制要避免故障导致失效。平时故意触发故障可以提高容错机制的能力，  
- **硬件故障**在大规模的集群中很常见，从硬盘崩溃到机房断电，不得不考虑。硬盘本身**平均无故障时间(MTTF mean time to failure)** 约为10-50年,10000个硬盘的集群中，平均每天都有1个硬盘故障。硬件冗余是最常见的解决方法，如RAID和后备电源。云平台这类新应用在设计时就优先考虑**灵活性**和**弹性**,而非单机可靠性，那么在硬件冗余之上加入软件容错机制就在容忍硬件故障上更进一步了。  
- **软件错误**相比于硬件故障更加系统性，这种内部的**系统性错误(systematic error)**更难以预料，而且可能范围更大，如千年虫bug，失控进程占用过多资源，但这种程序中根本的错误没有完美的解决办法，只能用生产中更多的测试，更多的监控等小办法缓解。  
- **人为错误**，运维配置错误是导致服务中断的首要原因，需要在设计，测试，监控，配置，乃至管理方面优化。  

##### 可扩展性
- 可扩展性是指系统应对负载增长的能力。==这不是一个静态的属性，如某些变量可扩展，某些不可，那样讨论没有意义；而是一个动态的解决方案，即“如果系统以某种方式（用户量增加，用户数据累积）增长，如何应对？”，或者“如何增加计算资源处理额外的负载。”==  
- **描述负载**
    - 负载可以用**负载参数(load parameters)** 来描述，具体却取决于架构，如每秒请求，数据库读写比率，活跃用户数量等等，平均情况和极端情况哪个是优化目标也是依情况而定的。  
    - 用简化的推特为例来说明。推特的两个主要业务：发布推文（平均4.6k/s,峰值12k/s），查看时间线(平均300k/s)。
    - 每秒12k的写入还是容易处理的，而推特的扩展性挑战来自**扇出(fan-out)**，即每个用户关注了很多人，也被很多人关注。
    - 这种操作有两种实现方式
      1. 用关系数据库，用户查看主页时，查询关注的所有人发送推文按时间的合并。
      2. 为每个用户的主页维持一个缓存，像收件箱一样，一个用户发送推文时，对所有关注者的缓存发送这条推文。
    - 推特一开始使用方法1，后来转向方法2，因为发推文比查询主页操作少得多，因此应该在前者时做更多事情，后者时做更少。但是方法2也有缺点，就是一些粉丝很多的用户（名流），他们发的每一条推文都意味着对缓存的大量写入，这很难迅速完成。因此，推特最后主要采用方法2，但是对名流的推文采用方法1，最后将两者得到的时间线合并。  
- **描述性能**
  - 自然地，我们会希望知道两个问题：增加负载参数时
    1. 保持系统资源不变，系统性能将如何被影响。
    2. 保持系统性能不变，需要增加多少系统资源。
  - 对于hadoop这样的批处理系统，我们通常关心**吞吐量(throughput)**，即每秒处理的记录数量。对于在线系统，更重要的是**响应时间(response time)**，即发送请求到接受响应之间的时间。
  - ==现实中，响应时间不是一个数值，而是一个**数值分布**，因为各式各样的请求有不同的响应时间。== 我们因此统计一个服务的响应时间均值和百分位数（如99分位数，最慢的1%的响应时间）。异常值在这里是重要的，最慢的那些响应时间（**尾部延迟(tail latencies)**）非常重要，因为他们往往是数据最多，最有价值的客户。==亚马逊在描述内部服务时就用99.9分位数为准。==
  - 实践中，后端应用常常调用大量后端服务，此时用户请求响应时间由最慢的服务决定，也因此高分位数比平均速度更重要。
  - **排队延迟(queueing delay)**通常占了高分位点响应时间的很大一部分，服务器并行任务数有限，因此少量缓慢的请求就能阻碍后续请求，即**头部阻塞**效应.
  - 负载增加时，如果这是数量级的增长，那么各种方法很难起效，常常需要重新考虑架构了。**纵向扩展(scaliing up)**（转向更强大的机器）和**横向扩展(scaling out)**（将负载分散到许多小机器）是两种典型的思路，同时采用两种自然是最好的。 
  - **弹性(elastic)** 系统是指可以在检测到负载增加时自动增加计算资源，对应的是手动扩展。如果负载难以预测，弹性系统可能有用，否则手动扩展更简单可靠。
  - **无状态服务(stateless services)**是很容易跨机器部署的，而带状态数据系统的分布式部署则较为复杂。（当然，随着分布式系统越来越好，分布式数据系统可能成为未来的默认配置）
  - 大规模的系统架构通常是应用特定的，没有**万金油(magic scaling sauce)**。一个良好的可扩展架构，围绕着**假设(assumption)**建立：哪些操作常见？哪些罕见？
  
##### 可维护性
- == 软件的大部分开销不在最初的开发阶段，而是在持续的维护阶段 == ，包括修复漏洞，保持系统正常运行，调查失效，适配新平台，添加新功能等等。
- 但是，许多程序员不喜欢维护**祖传(legacy)** 系统，以下的设计原则可以减少维护期间的痛苦：
  1. 可操作性
  2. 简单性
  3. 可演化性
- **可操作性**：使运维团队易于保持系统平稳运行
  - 良好的可操作性意味着更轻松的日常工作，则运维团队能专注于高价值的事情。
- **简单性**：管理复杂度
  - 随着项目越来越大，代码往往变得非常复杂，难以理解。**复杂度(complexity)** 的各种症状如纠结的依赖关系，不一致的命名和术语，需要绕开的特例。用于消除额外复杂度的最好工具是**抽象(abstraction)**.
- **可演化性**：适应变化
  - 数据系统层面的敏捷性，适应在频繁变化的环境中开发软件。

  
